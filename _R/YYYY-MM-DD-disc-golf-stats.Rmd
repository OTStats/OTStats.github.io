---
  title: "Webscrape Disc Golf Stats"
  subtitle: part
  date: YYYY-MM-DD
  category: cat
  tags: [sports, rstats]
---

A few months into my (2020) quarantine one of my neighbors took me disc golfing. It wasn't an entirely new experience for me, as I had gone a few times when I was younger, but this time around I was interested. In the days that followed I watched previous PDGA (Professional Disc Golf Association) events -- catching a glimpse of how the discs are **supposed** to behave. The next week I bought a starter pack. My fascination has since continued...

------------------------------------------------------------------------

As I've gone on to watch a few of the professional disc golf events I've gotten familiar with some of the pros currently at the top of the game. Players such as Paul Mcbeth, Ricky Wysocki, Eagle McMahon, and Calvin Heimburg were regularly appearing in the final rounds of events of the men's events, and Paige Pierce continually dominated the women's division. I began to wonder how much these professionals have earned (specifically, their winnings from each of these tournaments; I'm sure some make a decent amount from endorsements). Luckily the PDGA, the main governing body of professional disc golf, tracks most of this information. The PDGA website [Player Statistics](https://www.pdga.com/players/stats) page tracks annual earnings, ratings, and points for players for all PDGA-sanctioned events back to 1979. There didn't appear to be any convenient way for me to compare these player's earnings over time so I saw this as an opportunity to practice webscraping.

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

Let's start by just scraping a single page to understand the structure of the webpage (i.e. find the table within the HTML code). It took me about an hour to remind myself of how to use inspect to find the breadcrumb path to the html table, and I decided against including documentation about the process here (if this is new to you, there are plenty of resources online about how that provide better documentation than I could).

```{r load libraries}
library(tidyverse)
library(polite)
library(rvest)
library(xml2)
# Read single page, rankings for 2019
url <- "https://www.pdga.com/players/stats?Year=2019&player_Class=All&Gender=All&Bracket=All&continent=All&Country=All&StateProv=All&page=0"

url %>% 
  bow() %>% 
    scrape() %>% 
    html_node("body") %>% 
    xml2::xml_find_first("//table") %>% 
    html_table() %>% 
    as_tibble()

```

Voila. Now, a note about the actual URL string. The actual base URL for the PDGA Player Stats page is [*https://www.pdga.com/players/stats*](https://www.pdga.com/players/stats) *--* much shorter than in the code snippet above. After playing around with a few of the filters I found that they would also propagate the URL. So with some help from `purrr`, I could systematically pass a vector of years and a vector of page numbers to scrape PDGA player stats. First I can try scraping the top 100 players from 2019 -- which would mean that I'd need to scrape pages 0 through 4. I can supply a base URL, clarifying *Year=2019*, and finish the URL string with *page=*, only to paste the base to a vector from 0 to 4, and map a predefined function to scrape the page as I just did.

```{r top 100 from 2019}

base_2019 <- "https://www.pdga.com/players/stats?Year=2019&player_Class=All&Gender=All&Bracket=All&continent=All&Country=All&StateProv=All&order=Prize&sort=desc&page="

scrape_page <- function(url) {
  url_session = bow(url)
  url_session %>% 
    scrape() %>% 
    html_node("body") %>% 
    xml2::xml_find_first("//table") %>% 
    html_table() %>% 
    as_tibble()
}

(pdga_2019_top_100 <- str_c(base_2019, 0:4) %>% 
  map_df(~ scrape_page(url = .)))

```

------------------------------------------------------------------------

Using the `cross` function from the `purrr` package, and a little code snippet in the function's documentation, I was able to easily come up with a small bit of code that did a lot. I define a function to that will

By running the following bit of code, I accomplish the following: 1) define a function (same as above) to that will politely scrape the PDGA website and extract the HTML table and convert it to a tibble, 2) create a vector of all URL combinations for years 2015 through 2020 and pages 0 through 5 of the PDGA Player Stats page, and 3) passes that vector to `map_df()` with the aforementioned `scrape_url` (Note: this part of the script can take a little while, mainly because `polite` is using proper web scraping etiquette; my understanding is that it takes some time off between scraping pages). The last little bits include some basic data cleaning (i.e. using `janitor::clean_names()` to clean up those variable names, and add a `cash_value` variable which converts the prize money from a character string to a numeric value).

*Note: For a simple use case, I decided to use two predefined filters to select the men's open division. I have future iterations in mind, which I'll about later.*

```{r}
# -- Load libraries
library(tidyverse)
library(polite)
library(rvest)
library(xml2)
library(janitor)

# Define a function to scrape the PDGA player stats page and get the stats table
scrape_page <- function(url) {
  url_session = bow(url)
  url_session %>% 
    scrape() %>% 
    html_node("body") %>% 
    xml2::xml_find_first("//table") %>% 
    html_table() %>% 
    as_tibble()
}


# Source help (https://purrr.tidyverse.org/reference/cross.html)
pdga_params <- list(first_url_part = "https://www.pdga.com/players/stats?Year=", 
           years = 2015:2020, 
           second_url_part = "&player_Class=1&Gender=Male&Bracket=MPO&continent=All&Country=All&StateProv=All&order=Prize&sort=desc&page=", 
           pages = 0:4)

pdga_raw_scrape <- pdga_params %>% 
  cross() %>% 
  map(lift(paste0)) %>% 
  unlist() %>% 
  map_df(~ scrape_page(url = .)) %>% 
  janitor::clean_names() %>% 
  mutate(cash_value = str_remove_all(cash, "\\$|,") %>% as.numeric())
```

At this point we can start asking and answering question with our data. For example, what players made the most money from PDGA sanctioned events from 2015 to 2020?

```{r}

pdga_raw_scrape %>% 
  group_by(name) %>% 
  summarize(total_cash = sum(cash_value)) %>% 
  arrange(desc(total_cash)) %>% 
  mutate_at("total_cash", scales::dollar_format()) %>% 
  head(10) %>% 
  knitr::kable("pipe")
```

```{r}
pdga_raw_scrape %>% 
  inner_join(pdga_raw_scrape %>% 
               group_by(name) %>% 
               summarize(total_cash = sum(cash_value)) %>% 
               arrange(desc(total_cash)) %>% 
               mutate_at("total_cash", scales::dollar_format()) %>% 
               head(4) %>% 
               mutate_at("name", factor)) %>% 
  group_by(name) %>% 
  ggplot(aes(x = year, y = cash_value)) + 
  geom_col() + 
  expand_limits(y = 0) + 
  facet_wrap(~name)
```

```{r, include=FALSE}
# -----------------. 
url <- "https://www.pdga.com/players/stats?Year=2019&player_Class=All&Gender=All&Bracket=All&continent=All&Country=All&StateProv=All&page=0"
num_of_records <- url %>% 
  xml2::read_html() %>% 
  html_node("body") %>% 
  html_children() %>% 
  xml2::xml_find_first("//div[contains(@class, 'view-footer')]") %>% 
  html_children() %>% 
  .[1] %>% 
  as.character() %>% 
  str_extract(., "(?<= of )(.[:digit:]+)") %>% 
  as.numeric()
# ---- determine the number of pages that would need to be read for 2019
(num_of_records %/% 20) + 1 * ((num_of_records %% 20) != 0)
polite::bow()
session <- bow(url)
session %>% 
  scrape() %>% 
  html_node("body") %>% 
  xml2::xml_find_first("//table") %>% 
  html_table() %>% 
  as_tibble()
session %>% 
  scrape() %>% 
  html_node("body") %>% 
  xml2::xml_find_first("//div[contains(@class, 'view-footer')]") %>% 
  html_children() %>% 
  as.character() %>% 
  str_extract(., "(?<= of )(.[:digit:]+)") %>% 
  as.numeric()
  
  # xml2::read_html() %>% 
  # html_children() %>% 
  # .[1] %>% 
  # as.character() %>% 
  # as.numeric()
base = "https://www.pdga.com/players/stats?Year=2019&player_Class=All&Gender=All&Bracket=All&continent=All&Country=All&StateProv=All&page="
scrape_page <- function(url) {
  url_session = bow(url)
  url_session %>% 
    scrape() %>% 
    html_node("body") %>% 
    xml2::xml_find_first("//table") %>% 
    html_table() %>% 
    as_tibble()
}
str_c(base, 0:5) %>% 
  map_df(~ scrape_page(url = .))
```

```{r}
base_1 = "https://www.pdga.com/players/stats?Year="
base_2 = "&player_Class=1&Gender=Male&Bracket=MPO&continent=All&Country=All&StateProv=All&order=Prize&sort=desc&page=0"
df <- str_c(base_1, 2010:2020, base_2) %>% 
  map_df(~ scrape_page(url = .))
df %>% 
  group_by(Name) %>% 
  mutate(Cash = str_remove_all(Cash, "\\$|,") %>% as.numeric()) %>% 
  summarize(total_cash = sum(Cash)) %>% 
  arrange(desc(total_cash))
```

```{r webscrape combos help}
# Source help (https://purrr.tidyverse.org/reference/cross.html)
df <- list(first = "www.google.com/", 
           year = 2010:2015, 
           second = "/page=", 
           page = 0:5)
df %>% 
  cross() %>% 
  map(lift(paste0)) %>% 
  unlist()
```

## Application

```{r}
# Source help (https://purrr.tidyverse.org/reference/cross.html)
df <- list(first = "https://www.pdga.com/players/stats?Year=", 
           year = 2010:2015, 
           second = "&player_Class=1&Gender=Male&Bracket=MPO&continent=All&Country=All&StateProv=All&order=Prize&sort=desc&page=", 
           page = 0:5)
raw_scrape <- df %>% 
  cross() %>% 
  map(lift(paste0)) %>% 
  unlist() %>% 
  map_df(~ scrape_page(url = .)) %>% 
  janitor::clean_names()
raw_scrape %>% 
  group_by(name) %>% 
  mutate(cash_value = str_remove_all(cash, "\\$|,") %>% as.numeric()) %>% 
  summarize(total_cash = sum(cash_value)) %>% 
  arrange(desc(total_cash))
```
